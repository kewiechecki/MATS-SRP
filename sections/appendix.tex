
\section{Progress}
\label{app:progress}

\input{tables/legend}
\input{tables/models}
\input{tables/datavis}
\input{tables/code}
\input{tables/publication}

\section{Notation}
\label{app:notation}
We use lowercase Latin characters to denote scalars, boldface lowercase characters to denote vectors, and capital Latin characters to denote matrices.
Subscripts indicate indices.
Because we will mostly be working with matrices in $\mathbb{R}$, we abbreviate $X : \mathbb{R}^{m \times n}$ as $X^{m \times n}$.
We use a circumflex to indicate a reconstruction of an input by a predictor.
Lowercase Greek characters indicate the parameters of a model.
Capital Greek letters indicate parameter spaces.
Function names are in monospace.

\subsection{Layer function notation}
$\fn^{n \to m}$ indicates a layer with input dimension $n$, output dimension $m$, and activation function $\fn$.

$\Conv^{m \times n \times c \to d}_{\fn}$ indicates a convolutional layer that takes a kernel of size 
$m \times n$ with $c$ channels and applies activation function $\fn$. 
It returns $d$ channels for each layer.

$\ConvT^{d \to m \times n \times c}_{\fn}$ indicates a deconvolution layer that takes an input with $d$ channels and returns a reconstructed $m \times n$ output with $c$ channels.

\section{Model architecture}

\subsection{Outer models}
\label{app:outer}
I trained two outer models (Fig. \ref{fig:outermodel}.
Both used an identical convolutional encoder with a 3 neuron information bottleneck.
I first trained the encoder-classifier pair to force the model to represent more than 3 classifications in the bottleneck activations.
I then trained the decoder to reconstruct the images from the latent representation. 

\begin{figure}
     \begin{subfigure}[b]{0.17\textwidth}
        \input{tikz/outerencoder}
         \caption{}
         \label{fig:outerencoder}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.17\textwidth}
        \input{tikz/outerclassifier}
         \caption{}
         \label{fig:outerclassifier}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.17\textwidth}
         \input{tikz/outerdecoder}
         \caption{}
         \label{fig:outerdecoder}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.17\textwidth}
         \input{tikz/outerclassifier_inf}
         \caption{}
         \label{fig:outerenc_inf}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.17\textwidth}
         \input{tikz/outerencoder_inf}
         \caption{}
         \label{fig:outerenc_inf}
     \end{subfigure}
     \caption{Outer model architecture. (\ref{fig:outerencoder}) Encoder submodule shared by both outer models.
     (\ref{fig:outerclassifier}) Classifier submodule.
     (\ref{fig:outerdecoder}) Decoder submodule.
     (\ref{fig:outerclassifier}) Outer classifier forward pass.
     (\ref{fig:outerenc_inf}) Outer decoder forward pass.
     }
     \label{fig:outermodel}
\end{figure}

\subsection{Inner models}
\label{app:inner}

\subsubsection{SAE}
I trained an SAE with a 27 neuron embedding.
Implementation details can be found at 
\url{https://github.com/kewiechecki/SAE}.

\subsubsection{PSAE}
The partitioned sparse autoencoder (PSAE) was identical to the SAE except for the addition of a 12 neuron classifier.

\subsubsection{DeePWAK}
This architecture was identical to the PSAE except for the addition of a nonlinear decoder layer.
Moreover, training it does not include an \textsf{L1} loss.
Implementation details can be found at 
\url{https://github.com/kewiechecki/DeePWAK}.

\section{Self-supervised denoising}
\label{app:bgd}

\subsection{Denoising the data explains the data}
\label{app:ntos}
For a large class of denoising functions, it is possible to find optimal parameters using only unlabeled noisy data\cite{batson2019noise2self}.
$\ntos$ gives a near-maximally-general optimization target for hyperparameter search.


Let $J \in \mathcal{J}$ be independent partitions of noisy data $X$.
Let $\mathcal{F}(\theta)$ be a family of predictors of $X_J$ with tunable parameters
$\theta \in \Theta$ that depends on its complement $X_{J^C}$

\begin{equation}
  \hat{X}_J=\mathcal{F}(\theta)(X_{J^C})
\end{equation}

In other words, $\mathcal{F}$ predicts each data point $X_J$ from some subset of the data excluding $X_J$. 

  The optimal $\theta$ is given by

\begin{equation}
  \label{eq:ntos}
  \ntos_\theta^{\Theta}[\mathcal{F}(\theta),X] := \argmin_\theta^{\Theta}[\sum_{J}^{\mathcal{J}}\mathbb{E}[X_J-\mathcal{F}(\theta)(X_{J^C})]^2]
\end{equation}

\subsection{Diffusion with weighted affinity kernels}

$\ntos$ is particularly useful for finding optimal parameters for generating a graph\cite{tjarnberg2021}.
(see Appendix \ref{app:DEWAKSS})
The adjacency matrix $G$ of any graph can be treated as a transition matrix (or weighted affinity kernel) by setting the diagonal to 0 and normalizing columns to sum to 1. We call this the \WAK function (Algorithm \ref{alg:WAK}).

For each value in data $X$, an estimate is calculated based on its neighbors in the graph. This can be expressed as matrix multiplication.

\begin{equation}
  \label{eq:WAK}
\hat{X} := \WAK(G)X^\top
\end{equation}


\subsection{Partitioned weighted affinity kernels} 
\label{app:pwak}

Though DEWAKSS uses a $k$-NN graph, any adjacency matrix will do.
A clustering can be expressed as a graph where points within a cluster are completely connected and clusters are disconnected.

Let $K^{k \times n}$ be a matrix representing a clustering of $n$ points into $k$ clusters. Let each column be a 1-hot encoding of a cluster assignment for each point. We can obtain a partition matrix $P \in \mathbb{R}^{n \times n}$ by what we'll call the \textit{partitioned weighted affinity kernel} (\PWAK) function.

\begin{equation}
  \label{eq:PWAK}
  \PWAK(K) := \WAK(K^\top K)
\end{equation}

This lets us define a loss function

\begin{equation}
  \mathcal{L}_{\mathPWAK}(K,X) := \mathbb{E}[\mathPWAK(K)X^\top - X]^2
\end{equation}


\PWAK can be extended to soft cluster assignment, making it possible to learn $K$ via SGD.
We will refer to a model of this sort as a \Partitioner to emphasize that while it returns logits corresponding to classifications, there are no labels on the training data.

There is no accuracy measure separate from the decoder loss.
The partitioner simply tries to find the best $P$ for minimizing loss of the decoded output.

The only hyperparameters are the maximum number of clusters, the neural net architecture, and the training hyperparameters.
Because $PX^\top$ is $\mathcal{J}$-invariant, this classifier will converge on a solution less than the maximum $k$.
Intuitions from transformers may be helpful in visualizing why this works.
Informally, $P$ can be equated to position-independent attention with data points as tokens and the batch size as the context window.
Attentive readers may make a connection between masking the diagonal and BERT.

We can now train a model to classify unlabeled data into an undefined number of clusters with no prior distribution in $\mathcal{O}(n^2)$ time!

\begin{figure}
    \centering
    \includegraphics{fig/pwak.pdf}
    \caption{Example of a PSAE forward pass. Indices for subfigures are given as \textsf{(column,row)}.
    \textsf{(1,2)} and \textsf{(2,1)} show the predicted clusters.
    \textsf{(2,2)} is the partition matrix, obtained by applying the \PWAK transformation to the do product of
    \textsf{(1,2)} and \textsf{(2,1)}.
    \textsf{(3,2)} shows the embeddings obtained by the inner encoder.
    \textsf{(2,3)} shows the dot product of \textsf{(2,2)} and \textsf{(3,2)}, which corresponds to the denoised embeddings.
    \textsf{(3,1)} shows the dot product of \textsf{(2,1)} and \textsf{(3,2)}. This corresponds to finding the centroid of
    each cluster.
    \textsf{(1,3)} shows the dot product of \textsf{(1,2)} and \textsf{(2,3)}. This gives the denoised centroids.
    Presenting the data this way suggests an obvious optimization:
    instead of calculating the embeddings for each batch, can we just learn the cluster centroids?
    }
    \label{fig:pwak}
\end{figure}

\section{Bisemanticity in microscopy data}
\label{app:micro}
\secsubhead{
This section is especially disorganized. I was uncertain whether to include any of these results.
I ultimately decided to include a bare minimum to convey my thoughts on the link between 
bisemantic features and latent decision trees\cite{aytekin2022neural}.
}

Most of the theory for this project was originally developed for unsupervised phenotype detection
from microscopy images.
For these data the implementation of the encoder and decoder differed from a canonical SAE,
but the inner model still conformed to the spec in Fig. \ref{fig:deepwak}.
A larger difference was that I used an ensemble method consisting of 5 encoder/classifier heads.
Despite these differences, I think these results complement the MNIST results in a few ways.

\begin{figure}
    \centering
    \input{tikz/DeePWAKBlock}
    \caption{Multihead DeePWAK. Each head is as shown in Fig. \ref{fig:deepwak}.
    The final result is obtained through a linear combination of the outputs 
    of each head.}
    \label{fig:block}
\end{figure}

\begin{enumerate}
    \item It is a much smaller, less well-characterized data set
    \item The information content is nonobvious
    \item It's an independent toy model of feature aggregation: there is lots of redundant information,
    but the relations are nonobvious
    \item Linear methods fail to characterize the features well
\end{enumerate}

I mostly want to draw your attention to \ref{fig:blockE}, which gives a striking example of bisemanticity.
\textsf{Heads2-5} each seem to distinguish a All of the more nuanced discrimination happens in \textsf{Head1}.
This is suggestive that the model may in fact be learning a tree structure to split the data along major features. These major features effectively split experimenter-labeled phenotypes (Fig. \ref{fig:hyper}). 

It splits the data into almost but not quite the maximum number of clusters, indicating our choice of $k$ is close to the latent concept space for (this projection of) the data.
Fascinatingly, the embeddings are even sparser.

\begin{figure}
  \includegraphics[width=\textwidth]{params.pdf}
    \caption{Preprocessed microscopy data from 1853 embryos.
    I took a very crude approach to preprocessing, 
    extracting 114 ad hoc statistics with no consideration for redundancy or significance.
    The seeming poor quality of this data set hides a surprising amount of structure.
    }
    \label{fig:params}
\end{figure}

\begin{figure}
  %\begin{subfigure}[b]{0.5\textwidth}
  \includegraphics[width=\textwidth]{embeddings.pdf}
    \caption{DeePWAK learns sparse embedding values. 
    Each head actually has 14 embeddings, but for all but \textsf{Head1}, 
    they converge to two embeddings. 
    }
    \label{fig:blockE}
\end{figure}
