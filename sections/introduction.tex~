
\section{Threat Model}

I'm most worried about AGI arising from a sudden capabilities jump.
I'm less worried about AGI arising from incremental improvements in existing foundation models.
My reasoning is that
\begin{enumerate}
\item \paragraph{Alignment in the incremental scenario is less neglected}
  In this scenario, the incentives pushing capabilities will also push alignment.
  \label{threat:neglected}
\item \paragraph{Alignment in the incremental scenario is relatively tractable}
  In this scenario, there is a direct path from alignment research on weak models
  to alignment of powerful models.
  \label{threat:tractable}
\item \paragraph{There \textit{will} be a fire alarm for incremental AGI}
  We will see AGI coming, giving (\ref{threat:neglected}) and (\ref{threat:tractable}) time to work.
\end{enumerate}

We have no idea how to aim a model (``We don't even know how to make a paperclipper'').

We are currently very confused about what a concept is.

\begin{enumerate} 
\item \paragraph{Universality implies some ontologies are more ``natural'' than others.}
  If reasoning about
\item \paragraph{A model's latent ontology is primarily data depedent.}
  Diverse architectures can learn the same task.
  Current frontier models are overparameterized relative to high quality training data.
\item \paragraph{RLHF only works because LLMs learn the latent ontology of the training set}
\end{enumerate}

From these assumptions, there are a few areas t
%Mechinterp has had a fair bit of success on weak models, but we don't know how to make insights generalize to powerful models.

\section{Theory of Change}
I aim to address the problems of how to
\begin{enumerate}
  \item formalize the notion of a model's latent concept space
  \item distill a model's latent concept space
  \item enforce decomposability of latent concept space
  \item enforce robustness of existing concepts as the dimensionality of the latent concept space increases
\end{enumerate}
  
My justification for this line of research is
\begin{enumerate}
\item Almost every alignment proposal is limited by the ability to point at a concept.
\item According to singular learning theory, any sufficiently parameterized model trained on the same data will converge on the same latent representation of the data.
\item Mechinterp has made significant advances in identifying and characterizing specific features of interest, but mapping the entire feature space of existing frontier models is intractable.
\item Almost every alignment proposal becomes much more tractable if we can reason about features in isolation.
\end{enumerate}

Suppose we can locate a desirable concept in a model's ontology. This could be 
\subsection{A theoretical model of ontology}

Rapid progress in scaling LLMs has made it increasingly clear that data rather than compute is the limiting factor on capabilities.
This was not at all obvious in hindsight and if we survive the coming decades it will profoundly reshape every aspect of philosophy.
From an alignment perspective, the universality of deep learning independent of architecture is strong evidence for the natural abstraction hypothesis.
This is very good news! We have evidence that for any given data there is an ``objective'' ground truth.

\paragraph{Why might this be the case?}
Singular learning theory offers an information theoretic/thermodynamic explaination.

\subsubsection{Denoising, compression, and clustering}
Ontologies must serve an information theoretic purpose.
I argue that their primary purpose is minimizing prediction loss through \textit{denoising}.
Which is to say, discarding information that cannot be used to make inferences.
I further posit that there are two mechanisms by which ontologies form: compression and clustering.

\paragraph{Compression}
I define ``compression'' as finding

\paragraph{Clustering}

These mechanisms are synergistic. Informally, I equate the two with PCA and SVD.
In my preliminary results, I hope to demonstrate that this intuition has (putative) empirical backing.

\subsection{Implications for ELK}


\subsection{Aimability}
If ontologies are ``natural'', we should be 

\subsubsection{Decomposing an ontology}
To ensure a powerful 
