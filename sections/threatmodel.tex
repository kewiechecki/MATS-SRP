
\section{Threat Model}

We don't currently know how to specify a \hyperlink{https://arbital.com/p/diamond_maximizer/}{robust goal for a model}.
Because goal space is large, we should expect
\hyperlink{https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/FkgsxrGf3QxhfLWHG}{goal misgeneralization as the default outcome}
\footnote{The most straightforward path to doom is for a goal-driven superintelligence to seize power to prevent its optimization target from being changed.
In slow takeoff or multipolar scenarios, there is also a ``decoherence'' risk, 
where comparably weak specialist optimizers disempower humanity without any coherent strategy.
Consider for instance superpersuaders which fall short of superintelligence.
A viral chatbot might have very simple objectives to optimize engagement and distribution of itself.
While such a scenario would be an especially 
\hyperlink{https://www.lesswrong.com/posts/mSF4KTxAGRG3EHmhb/ai-x-risk-approximately-ordered-by-embarrassment}{undignified} 
end for humanity,
it broadly fits with both natural language being much easier than anyone expected and
AI labs being much less safety conscious than anyone expected.}.
%\footnote{I perhaps arbitrarily classify this as a misalignment risk rather than a misuse risk 
%based on whether the optimization behavior was intended.}

We can't currently look inside a model and make any conclusions about its goals.
Mechanistic interpretability (mechinterp) is rapidly improving but is still nowhere near being scalable to frontier models.

To a large extent, deep learning models seem to learn \hyperlink{}{universal} features irrespective of architecture.
This implies the existence of at least a weak version of
\hyperlink{https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1}{natural abstractions},
which would be a very convenient shortcut for auditing a model.
Unfortunately, the abstractions a model learns seem highly sensitive to choice of training data, and are thus not necessarily universal in the limit.
Assuming that model capabilities will continue to primarily be driven by access to more data and more modalities, we cannot expect the abstractions learned by a weak model to remain robust in a more powerful model.
